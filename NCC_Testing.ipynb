{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NCC test\n",
    "Identify causal relations on MSCOCO dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. MSCOCO dataset\n",
    "subset of 99,309 MSCOCO images belonging to 20 Pascal object categories. \n",
    "resize (224x224, 짧은 쪽이 224 pixel로 rescale하고 224x224 centercrop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object categories in pascal voc\n",
    "categories = [\n",
    "    'airplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', \n",
    "    'dining table', 'dog', 'horse', 'motorcycle', 'person', 'potted plant', 'sheep', 'couch', 'train', 'tv'\n",
    "]\n",
    "# 16 matched categories in coco dataset\n",
    "# no matched in coco: aeroplane, motorbike, sofa, television\n",
    "# aeroplane - airplane, motorbike - motorcycle, sofa - couch, television - tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'data/coco'\n",
    "dataType='train2014' #'val2014'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
    "# initialize COCO api for instance annotations\n",
    "coco=COCO(annFile)\n",
    "# categories\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "coco_cats = {cat['id']: cat['name'] for cat in cats}\n",
    "\n",
    "# get all images containing given categories, select one at random\n",
    "idx = 0\n",
    "dataset = []\n",
    "for cat in categories:\n",
    "    catIds = coco.getCatIds(catNms=cat)\n",
    "    imgIds = coco.getImgIds(catIds=catIds)\n",
    "    print(f'{cat}: {len(imgIds)}')\n",
    "    for imgId in imgIds:\n",
    "        annIds = coco.getAnnIds(imgIds=imgId, catIds=catIds, iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "        label = {'imgId': imgId, 'categories': [], 'bbox': []}\n",
    "        for i, ann in enumerate(anns):\n",
    "            label['categories'].append(coco_cats[ann['category_id']])\n",
    "            label['bbox'].append(ann['bbox'])\n",
    "        \n",
    "        img_file_name = coco.loadImgs(imgId)[0]['file_name']\n",
    "        label['file_name'] = img_file_name\n",
    "        \n",
    "        # dataset[idx] = label\n",
    "        dataset.append(label)\n",
    "        idx += 1\n",
    "print(f'Number of images: {len(dataset)}')\n",
    "\n",
    "# dataType1 = dataType.reshape('2014','')\n",
    "# with open(f'data/coco/{dataType1}Ids.txt', 'w') as f:\n",
    "#     json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(imgid, category):\n",
    "    # load and display instance annotations\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    # load image\n",
    "    img = coco.loadImgs(imgid)[0]\n",
    "    I = io.imread(img['coco_url'])\n",
    "    ax.imshow(I); plt.axis('off')\n",
    "    # load annotations\n",
    "    annIds = coco.getAnnIds(imgIds=img['id'], \n",
    "                            catIds=coco.getCatIds(catNms=category), \n",
    "                            iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    for i, ann in enumerate(anns):\n",
    "        ax.text(anns[i]['bbox'][0], anns[i]['bbox'][1], coco_cats[anns[i]['category_id']], style='italic', \n",
    "                bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(393221, 'airplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCOCO image (bbox + label)\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, dataDir='data/coco', transform=None):\n",
    "        # dataDir = dataDir\n",
    "        dataType='train2014'\n",
    "        # annFile='{}/annotations/instances_{}.json'.format(dataDir, dataType)\n",
    "        self.img_path = f'{dataDir}/{dataType}/'\n",
    "\n",
    "        # initialize COCO api for instance annotations\n",
    "        # self.coco=COCO(annFile)\n",
    "\n",
    "        # json\n",
    "        with open('data/coco/trainIds.txt', 'r') as f:\n",
    "            self.dataset = json.load(f) # 'img_id', 'bbox', 'anns'\n",
    "        \n",
    "        # transform\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        # img = Image.open(self.img_path+data['file_name'])\n",
    "        object_img = read_image(self.img_path+data['file_name'])\n",
    "        context_img = read_image(self.img_path+data['file_name'])\n",
    "        \n",
    "        cats = torch.tensor([categories.index(x) for x in data['categories']])\n",
    "        \n",
    "        # img = draw_bounding_boxes(img, bboxs)\n",
    "        mask = torch.zeros(img.shape[1:], dtype=int)\n",
    "        for bbox in data['bbox']:\n",
    "            x1, x2 = int(bbox[1]), int(bbox[1]+bbox[3])+1\n",
    "            y1, y2 = int(bbox[0]), int(bbox[0]+bbox[2])+1\n",
    "            mask[x1:x2,y1:y2] = 1\n",
    "\n",
    "        mask = torch.tensor(mask>0, dtype=torch.uint8)\n",
    "        \n",
    "        objectImg = torch.where(mask>0, img, mask)\n",
    "        contextImg = torch.where(mask>0, mask, img)\n",
    "        \n",
    "        objectImg = T.ToPILImage()(objectImg)\n",
    "        contextImg = T.ToPILImage()(contextImg)\n",
    "                \n",
    "        if self.transform is not None:\n",
    "            objectImg = self.transform(objectImg)\n",
    "            contextImg = self.transform(contextImg)\n",
    "\n",
    "        return objectImg, contextImg, cats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    # All images are rescaled to ensure that \n",
    "    # their shorter side is 224 pixels long, \n",
    "    # then cropped to the central 224×224 square\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor()\n",
    "])\n",
    "dataset = COCODataset(transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Model for object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor():\n",
    "    model_ft = resnet18(pretrained=True)\n",
    "    # finetune\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    # modify classifier\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Flatten()\n",
    "    # features = model_ft._modules.get('avgpool')\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    #  train on Pascal VOC 2012 dataset!!\n",
    "    def __init__(self, in_features=512, hidden_dim=512):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.classifier(x)\n",
    "        # softmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "resnet = feature_extractor().to(device)\n",
    "\n",
    "classifier = Classifier()\n",
    "classifier.load_state_dict(torch.load('results/object_classifier.pt'))\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objectImg, contextImg, cats in dataloader:\n",
    "    # plt.imshow(img1[0].permute(1,2,0))\n",
    "    # plt.show()\n",
    "    o_features = resnet(objectImg)\n",
    "    c_features = resnet(contextImg)\n",
    "    \n",
    "    o_logodds = classifier(o_features)\n",
    "    c_logodds = classifier(c_features)\n",
    "    \n",
    "    pair = (o_features, o_logodds)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "037aa815787b8e961403192ef1347e583d17d9ee4b11ffc4eb0aa3b81a4b5c37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
