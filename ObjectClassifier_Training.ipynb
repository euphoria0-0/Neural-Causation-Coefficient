{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Object Classifier\n",
    "\n",
    "**Model to identify causal relationships**: \n",
    "feature extraction network (ResNet18) trained on ImageNet and \n",
    "classifier network (two 512-unit hidden layers) trained on Pascal VOC 2012"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Pascal VOC2012 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\USERS\\YEONG\\MINICONDA3\\ENVS\\NCC\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object categories in pascal voc\n",
    "categories = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', \n",
    "    'dining table', 'dog', 'horse', 'motorbike', 'person', 'potted plant', 'sheep', 'sofa', 'train', 'television'\n",
    "]\n",
    "categories = {categories[i]: i for i in range(len(categories))}\n",
    "categories['tvmonitor'] = categories['television']\n",
    "categories['diningtable'] = categories['dining table']\n",
    "categories['pottedplant'] = categories['potted plant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/voc\\VOCtrainval_11-May-2012.tar\n",
      "Extracting data/voc\\VOCtrainval_11-May-2012.tar to data/voc\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "dataset = VOCDetection('data/voc', image_set='train', download=True, transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Feature (ResNet) Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor():\n",
    "    model_ft = resnet18(pretrained=True)\n",
    "    # finetune\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    # modify classifier\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Flatten()\n",
    "    # features = model_ft._modules.get('avgpool')\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    #  train on Pascal VOC 2012 dataset!!\n",
    "    def __init__(self, in_features=512, hidden_dim=512):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.classifier(x)\n",
    "        # softmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier (w/ feature extractor) on Pascal VOC2012\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "device = 'cpu'\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\USERS\\YEONG\\MINICONDA3\\ENVS\\NCC\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|██████████| 10/10 [56:14<00:00, 337.41s/it]\n"
     ]
    }
   ],
   "source": [
    "resnet = feature_extractor().to(device)\n",
    "classifier = Classifier().to(device)\n",
    "# torch.save(classifier.state_dict(), 'results/object_classifier.pt')\n",
    "\n",
    "resnet.eval()\n",
    "classifier.train()\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.RMSprop(classifier.parameters())\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epoch)):\n",
    "    for batch in tqdm.tqdm(dataloader, leave=False):\n",
    "        image_batch = torch.empty((len(batch), 3, 224, 224))\n",
    "        target_batch = torch.empty((len(batch), 20))\n",
    "        for idx, (image, anns) in enumerate(batch):\n",
    "            image_batch[idx] = image\n",
    "\n",
    "            target, target_names = [], []\n",
    "            for obj in anns['annotation']['object']:\n",
    "                target.append(categories[obj['name']])\n",
    "                target_names.append(obj['name'][0])\n",
    "                # bboxs.append(obj['bndbox'])\n",
    "            target_batch[idx] = F.one_hot(torch.tensor(target), num_classes=20).sum(axis=0).reshape(1,-1)\n",
    "        \n",
    "        image_batch, target_batch = image_batch.to(device), target_batch.to(device)\n",
    "        features = resnet(image_batch)\n",
    "        logodds = classifier(features)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logodds, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), f'results/object_classifier_{num_epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48b1116e935963e9dfbbc6a7d059da40e2da6a2194e11fb98dfdf755c66c6cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
